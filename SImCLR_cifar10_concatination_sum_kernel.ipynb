{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SDe5C8jwk5e3",
    "outputId": "02cc16b5-d0b0-4498-b133-d433a5d166b1"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xfDxSvNEpl_R",
    "outputId": "df6db7fa-76b5-4452-eb82-822daaa19f4e"
   },
   "outputs": [],
   "source": [
    "from psutil import virtual_memory\n",
    "ram_gb = virtual_memory().total / 1e9\n",
    "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
    "\n",
    "if ram_gb < 20:\n",
    "  print('To enable a high-RAM runtime, select the Runtime > \"Change runtime type\"')\n",
    "  print('menu, and then select High-RAM in the Runtime shape dropdown. Then, ')\n",
    "  print('re-execute this cell.')\n",
    "else:\n",
    "  print('You are using a high-RAM runtime!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kNrs7vZwSNTS"
   },
   "outputs": [],
   "source": [
    "# !mkdir /saved_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4DW6a6Y8hWtT"
   },
   "source": [
    "# Imports and Utility functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AYyIZ6dphbN8"
   },
   "source": [
    "## <font color='orange'>Imports</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "caWtyJW40H3k"
   },
   "outputs": [],
   "source": [
    "#!pip3 install tensorboard --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yQrK-eB3mBAC"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shutil, time, os, requests, random, copy\n",
    "from itertools import permutations \n",
    "import seaborn as sns\n",
    "import imageio\n",
    "from skimage.transform import rotate, AffineTransform, warp, resize\n",
    "#import skvideo.io as vidio\n",
    "#from google.colab.patches import cv2_imshow\n",
    "from IPython.display import clear_output, Image, SVG\n",
    "import h5py\n",
    "#from tabulate import tabulate\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as tF\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.datasets as dset\n",
    "from torchvision import datasets, transforms, models\n",
    "#from torchviz import make_dot\n",
    "#from torchsummary import summary\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc, f1_score\n",
    "from sklearn.metrics import average_precision_score, precision_recall_curve, roc_auc_score, accuracy_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#import matplotlib.animation as animation\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j7ttFTj6pl_T"
   },
   "outputs": [],
   "source": [
    "from torch.hub import load_state_dict_from_url\n",
    "from typing import Type, Any, Callable, Union, List, Optional\n",
    "from torch import Tensor\n",
    "from collections import OrderedDict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nIqaJk_s0H3t",
    "outputId": "3fe41b71-e7a8-48f8-db1c-58562100d709"
   },
   "outputs": [],
   "source": [
    "np.random.seed(16)\n",
    "torch.manual_seed(16)\n",
    "#tf.random.set_seed(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lfdo-DDzQ9LD"
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zziaDy3rpl_T"
   },
   "outputs": [],
   "source": [
    "def set_seed(seed = 16):\n",
    "    np.random.seed(16)\n",
    "    torch.manual_seed(16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qhz7ofsF1QoX"
   },
   "source": [
    "#### CIFAR 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wget "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JJONIwKjpl_X",
    "outputId": "e0a54c86-f3b9-469a-e9c5-500e3bc735c5"
   },
   "outputs": [],
   "source": [
    "# !wget https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8sbhYkUDMnon"
   },
   "outputs": [],
   "source": [
    "# !tar -xf /cifar-10-python.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "--hbMxr3Mn1g"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yWp44kBgMn9D",
    "outputId": "03166ace-b0be-4d0f-d98d-3539a9368bfa"
   },
   "outputs": [],
   "source": [
    "train_files = ['data_batch_1','data_batch_2','data_batch_3','data_batch_4','data_batch_5']\n",
    "images = np.array([],dtype=np.uint8).reshape((0,3072))\n",
    "labels = np.array([])\n",
    "for tf in train_files:\n",
    "    data_dict = unpickle('./cifar-10-batches-py/'+tf)\n",
    "    data = data_dict[b'data']\n",
    "    images = np.append(images,data,axis=0)\n",
    "    labels = np.append(labels,data_dict[b'labels'])\n",
    "print(images.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gKO0K3hFMoEe",
    "outputId": "7ebf7ca3-da52-4ee4-d85b-28cb6c2443fa"
   },
   "outputs": [],
   "source": [
    "testimages = np.array([],dtype=np.uint8).reshape((0,3072))\n",
    "testlabels = np.array([])\n",
    "\n",
    "data_dict = unpickle('./cifar-10-batches-py/test_batch')\n",
    "data = data_dict[b'data']\n",
    "testimages = np.append(testimages,data,axis=0)\n",
    "testlabels = np.append(testlabels,data_dict[b'labels'])\n",
    "print(testimages.shape, testlabels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7nUOYZFEMoLz",
    "outputId": "0ef37c45-0b11-4b33-fb7d-8e657606e249"
   },
   "outputs": [],
   "source": [
    "images = images.reshape((-1,3,32,32))\n",
    "testimages = testimages.reshape((-1,3,32,32))\n",
    "\n",
    "print(images.shape, testimages.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "edReLh5LMoTB"
   },
   "outputs": [],
   "source": [
    "# images = images.astype(np.float)\n",
    "# testimages = testimages.astype(np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4H5FOWP6RtBa"
   },
   "outputs": [],
   "source": [
    "trimages = np.moveaxis(images,1,3)\n",
    "tsimages = np.moveaxis(testimages,1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5FAFkcmTRWV8"
   },
   "outputs": [],
   "source": [
    "# labels = labels.astype(np.int)\n",
    "# testlabels = testlabels.astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M0IGuzTGQmEJ"
   },
   "outputs": [],
   "source": [
    "lab_dict = {0:'airplane',1:'automobile',2:'bird',3:'cat',4:'deer',5:'dog',6:'frog',7:'horse',8:'ship',9:'truck'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "id": "hvQ9ElwuQHzR",
    "outputId": "52369f21-8d8f-47d6-e12b-62f9e84b943c"
   },
   "outputs": [],
   "source": [
    "plt.imshow(trimages[1]/255.0)\n",
    "plt.show()\n",
    "print(lab_dict[labels[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "42RiWwJZkUH5"
   },
   "source": [
    "### <font color='blue'>Performance Metrics</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kpT8np6skUH5"
   },
   "outputs": [],
   "source": [
    "#util_wk2\n",
    "def TP(y, pred, th=0.5):\n",
    "    pred_t = (pred > th)\n",
    "    return np.sum((pred_t == True) & (y == 1))\n",
    "\n",
    "\n",
    "def TN(y, pred, th=0.5):\n",
    "    pred_t = (pred > th)\n",
    "    return np.sum((pred_t == False) & (y == 0))\n",
    "\n",
    "\n",
    "def FN(y, pred, th=0.5):\n",
    "    pred_t = (pred > th)\n",
    "    return np.sum((pred_t == False) & (y == 1))\n",
    "\n",
    "def FP(y, pred, th=0.5):\n",
    "    pred_t = (pred > th)\n",
    "    return np.sum((pred_t == True) & (y == 0))\n",
    "\n",
    "def get_accuracy(y, pred, th=0.5):\n",
    "    tp = TP(y,pred,th)\n",
    "    fp = FP(y,pred,th)\n",
    "    tn = TN(y,pred,th)\n",
    "    fn = FN(y,pred,th)\n",
    "    \n",
    "    return (tp+tn)/(tp+fp+tn+fn)\n",
    "\n",
    "def get_prevalence(y):\n",
    "    return np.sum(y)/y.shape[0]\n",
    "\n",
    "def sensitivity(y, pred, th=0.5):\n",
    "    tp = TP(y,pred,th)\n",
    "    fn = FN(y,pred,th)\n",
    "    \n",
    "    return tp/(tp+fn)\n",
    "\n",
    "def specificity(y, pred, th=0.5):\n",
    "    tn = TN(y,pred,th)\n",
    "    fp = FP(y,pred,th)\n",
    "    \n",
    "    return tn/(tn+fp)\n",
    "\n",
    "def get_ppv(y, pred, th=0.5):\n",
    "    tp = TP(y,pred,th)\n",
    "    fp = FP(y,pred,th)\n",
    "    \n",
    "    return tp/(tp+fp)\n",
    "\n",
    "def get_npv(y, pred, th=0.5):\n",
    "    tn = TN(y,pred,th)\n",
    "    fn = FN(y,pred,th)\n",
    "    \n",
    "    return tn/(tn+fn)\n",
    "\n",
    "\n",
    "def get_performance_metrics(y, pred, class_labels, tp=TP,\n",
    "                            tn=TN, fp=FP,\n",
    "                            fn=FN,\n",
    "                            acc=get_accuracy, prevalence=get_prevalence, \n",
    "                            spec=specificity,sens=sensitivity, ppv=get_ppv, \n",
    "                            npv=get_npv, auc=roc_auc_score, f1=f1_score,\n",
    "                            thresholds=[]):\n",
    "    if len(thresholds) != len(class_labels):\n",
    "        thresholds = [.5] * len(class_labels)\n",
    "\n",
    "    columns = [\"Injury\", \"TP\", \"TN\", \"FP\", \"FN\", \"Accuracy\", \"Prevalence\",\n",
    "               \"Sensitivity\",\n",
    "               \"Specificity\", \"PPV\", \"NPV\", \"AUC\", \"F1\", \"Threshold\"]\n",
    "    df = pd.DataFrame(columns=columns)\n",
    "    for i in range(len(class_labels)):\n",
    "        df.loc[i] = [class_labels[i],\n",
    "                     round(tp(y[:, i], pred[:, i]),3),\n",
    "                     round(tn(y[:, i], pred[:, i]),3),\n",
    "                     round(fp(y[:, i], pred[:, i]),3),\n",
    "                     round(fn(y[:, i], pred[:, i]),3),\n",
    "                     round(acc(y[:, i], pred[:, i], thresholds[i]),3),\n",
    "                     round(prevalence(y[:, i]),3),\n",
    "                     round(sens(y[:, i], pred[:, i], thresholds[i]),3),\n",
    "                     round(spec(y[:, i], pred[:, i], thresholds[i]),3),\n",
    "                     round(ppv(y[:, i], pred[:, i], thresholds[i]),3),\n",
    "                     round(npv(y[:, i], pred[:, i], thresholds[i]),3),\n",
    "                     round(auc(y[:, i], pred[:, i]),3),\n",
    "                     round(f1(y[:, i], pred[:, i] > thresholds[i]),3),\n",
    "                     round(thresholds[i], 3)]\n",
    "\n",
    "    df = df.set_index(\"Injury\")\n",
    "    return df\n",
    "\n",
    "def bootstrap_metric(y, pred, classes, metric='auc',bootstraps = 100, fold_size = 1000):\n",
    "    statistics = np.zeros((len(classes), bootstraps))\n",
    "    if metric=='AUC':\n",
    "        metric_func = roc_auc_score\n",
    "    if metric=='Sensitivity':\n",
    "        metric_func = sensitivity\n",
    "    if metric=='Specificity':\n",
    "        metric_func = specificity\n",
    "    if metric=='Accuracy':\n",
    "        metric_func = get_accuracy\n",
    "    for c in range(len(classes)):\n",
    "        df = pd.DataFrame(columns=['y', 'pred'])\n",
    "        df.loc[:, 'y'] = y[:, c]\n",
    "        df.loc[:, 'pred'] = pred[:, c]\n",
    "        # get positive examples for stratified sampling\n",
    "        df_pos = df[df.y == 1]\n",
    "        df_neg = df[df.y == 0]\n",
    "        prevalence = len(df_pos) / len(df)\n",
    "        for i in range(bootstraps):\n",
    "            # stratified sampling of positive and negative examples\n",
    "            pos_sample = df_pos.sample(n = int(fold_size * prevalence), replace=True)\n",
    "            neg_sample = df_neg.sample(n = int(fold_size * (1-prevalence)), replace=True)\n",
    "\n",
    "            y_sample = np.concatenate([pos_sample.y.values, neg_sample.y.values])\n",
    "            pred_sample = np.concatenate([pos_sample.pred.values, neg_sample.pred.values])\n",
    "            score = metric_func(y_sample, pred_sample)\n",
    "            statistics[c][i] = score\n",
    "    return statistics\n",
    "\n",
    "def get_confidence_intervals(y,pred,class_labels):\n",
    "    \n",
    "    metric_dfs = {}\n",
    "    for metric in ['AUC','Sensitivity','Specificity','Accuracy']:\n",
    "        statistics = bootstrap_metric(y,pred,class_labels,metric)\n",
    "        df = pd.DataFrame(columns=[\"Mean \"+metric+\" (CI 5%-95%)\"])\n",
    "        for i in range(len(class_labels)):\n",
    "            mean = statistics.mean(axis=1)[i]\n",
    "            max_ = np.quantile(statistics, .95, axis=1)[i]\n",
    "            min_ = np.quantile(statistics, .05, axis=1)[i]\n",
    "            df.loc[class_labels[i]] = [\"%.2f (%.2f-%.2f)\" % (mean, min_, max_)]\n",
    "        metric_dfs[metric] = df\n",
    "    return metric_dfs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qQJUoL4zuNYJ"
   },
   "source": [
    "### <font color='blue'>Plotting Metrics</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wkLyllF2kUH8"
   },
   "source": [
    "#### <font color='red'>Accuracy</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8c0xP0kZNN84"
   },
   "outputs": [],
   "source": [
    "def plot_accuracy(tr_acc,val_acc):\n",
    "    # Plot training & validation accuracy values\n",
    "    plt.plot(tr_acc)\n",
    "    plt.plot(val_acc)\n",
    "    plt.title('Model accuracy',fontsize=10)\n",
    "    plt.ylabel('Accuracy',fontsize=10)\n",
    "    plt.xlabel('Epoch',fontsize=10)\n",
    "    plt.tick_params(axis='both', which='major', labelsize=10)\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left',prop={'size': 10})\n",
    "    plt.savefig('accuracy_plot.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aI_hg4wXkUH-"
   },
   "source": [
    "#### <font color='red'>Loss</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pOhp8lNpNOsb"
   },
   "outputs": [],
   "source": [
    "def plot_loss(tr_loss,val_loss):\n",
    "    # Plot training & validation loss values\n",
    "    plt.plot(tr_loss)\n",
    "    plt.plot(val_loss)\n",
    "    plt.title('Model loss',fontsize=10)\n",
    "    plt.ylabel('Loss',fontsize=10)\n",
    "    plt.xlabel('Epoch',fontsize=10)\n",
    "    plt.tick_params(axis='both', which='major', labelsize=10)\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left',prop={'size': 10})\n",
    "    plt.savefig('loss_plot.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bopi6WMmkUIA"
   },
   "source": [
    "#### <font color='red'>ROC</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TRMFODDikUIB"
   },
   "outputs": [],
   "source": [
    "def get_roc_curve(gt, pred, target_names):\n",
    "    for i in range(len(target_names)):\n",
    "        curve_function = roc_curve\n",
    "        auc_roc = roc_auc_score(gt[:, i], pred[:, i])\n",
    "        label = str(target_names[i]) + \" AUC: %.3f \" % auc_roc\n",
    "        xlabel = \"False positive rate\"\n",
    "        ylabel = \"True positive rate\"\n",
    "        a, b, _ = curve_function(gt[:, i], pred[:, i])\n",
    "        plt.figure(1, figsize=(7, 7))\n",
    "        plt.plot([0, 1], [0, 1], 'k--')\n",
    "        plt.plot(a, b, label=label)\n",
    "        plt.xlabel(xlabel)\n",
    "        plt.ylabel(ylabel)\n",
    "        plt.legend(loc='upper center', bbox_to_anchor=(1.3, 1),\n",
    "                       fancybox=True, ncol=1)\n",
    "        plt.savefig('ROC_Curve.png')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DBxj7WvNkUIC"
   },
   "source": [
    "#### <font color='red'>Precision and Recall</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Inw3SVknkUID"
   },
   "outputs": [],
   "source": [
    "def get_PR_curve(gt, pred, target_names):\n",
    "    for i in range(len(target_names)):\n",
    "        precision, recall, _ = precision_recall_curve(gt[:, i], pred[:, i])\n",
    "        average_precision = average_precision_score(gt[:, i], pred[:, i])\n",
    "        label = str(target_names[i]) + \" Avg.: %.3f \" % average_precision\n",
    "        plt.figure(1, figsize=(7, 7))\n",
    "        plt.step(recall, precision, where='post', label=label)\n",
    "        plt.xlabel('Recall')\n",
    "        plt.ylabel('Precision')\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.legend(loc='upper center', bbox_to_anchor=(1.3, 1),\n",
    "                       fancybox=True, ncol=1)\n",
    "        plt.savefig('Precision_and_Recall_curve.png')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sz7kNHQxkUIE"
   },
   "source": [
    "#### <font color='red'>Confusion Matrix</font>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mviMdmIlkUIG"
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true,y_pred,class_labels):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=class_labels)\n",
    "    cm_sum = np.sum(cm, axis=1, keepdims=True)\n",
    "    cm_perc = cm / cm_sum.astype(float) * 100\n",
    "    annot = np.empty_like(cm).astype(str)\n",
    "    nrows, ncols = cm.shape\n",
    "    for i in range(nrows):\n",
    "        for j in range(ncols):\n",
    "            c = cm[i, j]\n",
    "            p = cm_perc[i, j]\n",
    "            if i == j:\n",
    "                s = cm_sum[i]\n",
    "                annot[i, j] = '%.1f%%\\n%d/%d' % (p, c, s)\n",
    "            elif c == 0:\n",
    "                annot[i, j] = ''\n",
    "            else:\n",
    "                annot[i, j] = '%.1f%%\\n%d' % (p, c)\n",
    "    cm = pd.DataFrame(cm, index=class_labels, columns=class_labels)\n",
    "    cm.index.name = 'Actual'\n",
    "    cm.columns.name = 'Predicted'\n",
    "    fig, ax = plt.subplots(figsize=(60,60))\n",
    "    sns.set(font_scale=3.0) # Adjust to fit\n",
    "    sns.heatmap(cm, cmap= \"YlGnBu\", annot=annot, fmt='', ax=ax)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=10)  # Adjust to fit\n",
    "    ax.xaxis.set_ticklabels(class_labels)\n",
    "    ax.yaxis.set_ticklabels(class_labels)\n",
    "    fig.savefig('Confusion_Matrix.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AZTz4lvkkUII"
   },
   "source": [
    "#### <font color='red'>Performance Metrics with Errorbars</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i611AmBdkUII"
   },
   "outputs": [],
   "source": [
    "#04_03_Errorbar.ipynb\n",
    "def plot_perf_metrics_errbars(y,pred,class_labels):\n",
    "    metric_dfs = get_confidence_intervals(y,pred,class_labels)\n",
    "    metrics = metric_dfs.keys()\n",
    "    fig,axs = plt.subplots(len(metrics),1,sharey=True)\n",
    "    for i in range(len(metrics)):\n",
    "        ci = metric_dfs[metric][['Mean '+metrics[i]+' (CI 5%-95%)']].values\n",
    "        ci_mean,ci_ints = np.array([c[0].split(' ') for c in ci]).T\n",
    "        ci_mean = ci_mean.astype(float)\n",
    "        ci_min,ci_max = np.array([ci_ints.strip('()').split('-')]).astype(float)\n",
    "        ci_err = (ci_max-ci_min)/2\n",
    "        \n",
    "        axs[i].errorbar(class_labels,ci_mean,yerr=ci_err,capsize=5,fmt='dk')\n",
    "        axs[i].set_ylabel(metrics[i])\n",
    "    fig.savefig('Performance_Metrics_95percentCI.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R7FekhZepl_b"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NkJpjkgQ4bOh"
   },
   "source": [
    "## Data Generator for Contrastive Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uxVU4yN2fmNf"
   },
   "source": [
    "### Augmentations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GGxeZGL5pl_f"
   },
   "source": [
    "#### Cutout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uevy4eqIpl_f"
   },
   "outputs": [],
   "source": [
    "class Cutout(nn.Module):\n",
    "    def __init__(self, seed = 0):\n",
    "        self.seed = seed\n",
    "        \n",
    "    def get_start_index(self,L):\n",
    "        return np.random.randint(L)\n",
    "    \n",
    "    def __call__(self,frame):\n",
    "        channels, h, w = frame.shape\n",
    "        #print(frames.shape)\n",
    "        size = h//4\n",
    "        n_squares = np.random.randint(1,3,1)[0]\n",
    "        new_image = frame\n",
    "        for _ in range(n_squares):\n",
    "            y = np.clip(self.get_start_index(h), size // 2, h - size//2)\n",
    "            x = np.clip(self.get_start_index(w), size // 2, w - size//2)\n",
    "            \n",
    "            y1 = np.clip(y - size // 2, 0, h)\n",
    "            y2 = np.clip(y + size // 2, 0, h)\n",
    "            x1 = np.clip(x - size // 2, 0, w)\n",
    "            x2 = np.clip(x + size // 2, 0, w)\n",
    "            new_image[:, y1:y2,x1:x2] = 0\n",
    "        return new_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pQNjkxBwpl_f"
   },
   "source": [
    "#### Gaussian Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yUy6waPkpl_f"
   },
   "outputs": [],
   "source": [
    "class AddGaussianNoise(nn.Module):\n",
    "    def __init__(self, mean=0., std=1.):\n",
    "        self.std = std\n",
    "        self.mean = mean\n",
    "        \n",
    "    def __call__(self, tensor):\n",
    "        return tensor + torch.randn(tensor.size()) * self.std + self.mean\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WEggCJ-p1T3h"
   },
   "source": [
    "### Datagen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eZHyhUo0SF9w"
   },
   "source": [
    "###### CIFAR 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1F49w-IATyCl",
    "outputId": "2f8eb9d9-ae7a-441f-b960-9f0256db450f"
   },
   "outputs": [],
   "source": [
    "MEAN = np.mean(images[:40000]/255.0,axis=(0,2,3),keepdims=True)\n",
    "STD = np.std(images[:40000]/255.0,axis=(0,2,3),keepdims=True)\n",
    "\n",
    "print(MEAN, STD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wGLeZT4lSGV0"
   },
   "outputs": [],
   "source": [
    "class C10DataGen(Dataset):\n",
    "    def __init__(self,phase,imgarr,s = 0.5):\n",
    "        self.phase = phase\n",
    "        self.imgarr = imgarr\n",
    "        self.s = s\n",
    "        self.transforms = transforms.Compose([transforms.RandomHorizontalFlip(0.5),\n",
    "                                              transforms.RandomResizedCrop(32,(0.8,1.0)),\n",
    "                                              transforms.Compose([transforms.RandomApply([transforms.ColorJitter(0.8*self.s, \n",
    "                                                                                                                 0.8*self.s, \n",
    "                                                                                                                 0.8*self.s, \n",
    "                                                                                                                 0.2*self.s)], p = 0.8),\n",
    "                                                                  transforms.RandomGrayscale(p=0.2)\n",
    "                                                                 ])])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.imgarr.shape[0]\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        \n",
    "        x = self.imgarr[idx] \n",
    "        #print(x.shape)\n",
    "        x = x.astype(np.float32)/255.0\n",
    "\n",
    "        x1 = self.augment(torch.from_numpy(x))\n",
    "        x2 = self.augment(torch.from_numpy(x))\n",
    "        \n",
    "        x1 = self.preprocess(x1)\n",
    "        x2 = self.preprocess(x2)\n",
    "        \n",
    "        return x1, x2\n",
    "\n",
    "    #shuffles the dataset at the end of each epoch\n",
    "    def on_epoch_end(self):\n",
    "        self.imgarr = self.imgarr[random.sample(population = list(range(self.__len__())),k = self.__len__())]\n",
    "\n",
    "    def preprocess(self,frame):\n",
    "        frame = (frame-MEAN)/STD\n",
    "        return frame\n",
    "    \n",
    "    #applies randomly selected augmentations to each clip (same for each frame in the clip)\n",
    "    def augment(self, frame, transformations = None):\n",
    "        \n",
    "        if self.phase == 'train':\n",
    "            frame = self.transforms(frame)\n",
    "        else:\n",
    "            return frame\n",
    "        \n",
    "        return frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cIqfcSTWpl_g"
   },
   "source": [
    "#### DataGen Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3CLG1XLipl_g"
   },
   "outputs": [],
   "source": [
    "dg = C10DataGen('train',images) #train_df)\n",
    "dl = DataLoader(dg,batch_size = 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "2bdppfUYpl_g",
    "outputId": "f65cfd59-6875-4029-9968-db1f8e740c00",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#MEAN = np.array([0.485, 0.456, 0.406]).reshape((1,1,3))\n",
    "#STD = np.array([0.229, 0.224, 0.25]).reshape((1,1,3))\n",
    "fig,axs = plt.subplots(16,2,figsize=(16,128))\n",
    "row = 0\n",
    "col = 0\n",
    "for step,(x1,x2) in enumerate(dl):\n",
    "    \n",
    "    for i in range(16):\n",
    "        #print(x1[i]*STD + MEAN)\n",
    "        #print(x2[i]*STD + MEAN)\n",
    "        axs[i,0].imshow((x1[i]*STD + MEAN).squeeze().permute(1,2,0).numpy())\n",
    "        axs[i,1].imshow((x2[i]*STD + MEAN).squeeze().permute(1,2,0).numpy())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C2qO7Q43pl_g"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eRZ-ey1_2mPZ"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mPKTwFQl7a_h"
   },
   "source": [
    "### Final Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2Wgq6cQy0H7t"
   },
   "outputs": [],
   "source": [
    "set_seed(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PYkMcwWYmawC"
   },
   "outputs": [],
   "source": [
    "class Identity(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Identity, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "class LinearLayer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_features,\n",
    "                 out_features,\n",
    "                 use_bias = True,\n",
    "                 use_bn = False,\n",
    "                 **kwargs):\n",
    "        super(LinearLayer, self).__init__(**kwargs)\n",
    "\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.use_bias = use_bias\n",
    "        self.use_bn = use_bn\n",
    "        \n",
    "        self.linear = nn.Linear(self.in_features, \n",
    "                                self.out_features, \n",
    "                                bias = self.use_bias and not self.use_bn)\n",
    "        if self.use_bn:\n",
    "             self.bn = nn.BatchNorm1d(self.out_features)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.linear(x)\n",
    "        if self.use_bn:\n",
    "            x = self.bn(x)\n",
    "        return x\n",
    "\n",
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_features,\n",
    "                 hidden_features,\n",
    "                 out_features,\n",
    "                 head_type = 'nonlinear',\n",
    "                 **kwargs):\n",
    "        super(ProjectionHead,self).__init__(**kwargs)\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.hidden_features = hidden_features\n",
    "        self.head_type = head_type\n",
    "\n",
    "        if self.head_type == 'linear':\n",
    "            self.layers = LinearLayer(self.in_features,self.out_features,False, True)\n",
    "        elif self.head_type == 'nonlinear':\n",
    "            self.layers = nn.Sequential(\n",
    "                LinearLayer(self.in_features,self.hidden_features,True, True),\n",
    "                nn.ReLU(),\n",
    "                LinearLayer(self.hidden_features,self.out_features,False,True))\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.layers(x)\n",
    "        return x\n",
    "\n",
    "class PreModel(nn.Module):\n",
    "    def __init__(self,base_model,base_out_layer):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "        self.base_out_layer = base_out_layer\n",
    "        \n",
    "        #PRETRAINED MODEL\n",
    "        self.pretrained = models.resnet50(pretrained=True)\n",
    "        \n",
    "        self.pretrained.conv1 = nn.Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
    "        self.pretrained.maxpool = Identity()\n",
    "        \n",
    "        self.pretrained.fc = Identity()\n",
    "        \n",
    "        for p in self.pretrained.parameters():\n",
    "            p.requires_grad = True\n",
    "        \n",
    "        self.projector = ProjectionHead(2048, 2048, 128)\n",
    "    \n",
    "\n",
    "    def forward(self,x):\n",
    "        out = self.pretrained(x)\n",
    "        \n",
    "        xp = self.projector(torch.squeeze(out))\n",
    "        \n",
    "        return xp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 83,
     "referenced_widgets": [
      "6213f8efc85b41b8b9c23375c416057e",
      "1a700b2fc234400498581d2081c89153",
      "67a457df909442e58782494206d86974",
      "159070707f784605a2336cffb647d02f",
      "a38a8031fa3f45feaa930bc12f2efc28",
      "c3777ffbc9f941e5a52e11a7e2a59ec0",
      "f47647ab20454613b70c4f78c6175c7b",
      "6a7b6912ecb3418da2aa22ef34d9ae5a"
     ]
    },
    "id": "CHJYDr2kFG1J",
    "outputId": "23e6eaef-2fa7-430f-8d2d-bec5cfc29dae",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = PreModel('resnet50','avgpool')\n",
    "model = model.to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DHRLkMgDpl_h",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.DataParallel(model, device_ids=[0])  # Use multiple GPUs (cuda:0 and cuda:1)\n",
    "model = model.to('cuda:0') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.random((8, 3, 224, 224))  # Sample input data\n",
    "x = torch.tensor(x, dtype=torch.float32).to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S4jCRPqKpl_i"
   },
   "outputs": [],
   "source": [
    "# x = np.random.random((3,224,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_P9kuZf1FbXk",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# out = model(torch.tensor(x, device = 'cuda:0', dtype = torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p2ZXrAypFN1r",
    "outputId": "26a61aca-47c8-44ba-b669-fb07cae2c657"
   },
   "outputs": [],
   "source": [
    "# out[0].shape, out[1].shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZEaRSe00ZRIO"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YzZR6tLnFN7U"
   },
   "outputs": [],
   "source": [
    "def load_optimizer(arg_optimizer, model, batch_size):\n",
    "\n",
    "    scheduler = None\n",
    "    if arg_optimizer == \"Adam\":\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)  # TODO: LARS\n",
    "    elif arg_optimizer == \"LARS\":\n",
    "        # optimized using LARS with linear learning rate scaling\n",
    "        # (i.e. LearningRate = 0.3 × BatchSize/256) and weight decay of 10−6.\n",
    "        learning_rate = 0.3 * batch_size / 256\n",
    "        optimizer = LARS(\n",
    "            [params for params in model.parameters() if params.requires_grad],\n",
    "            lr=0.1,\n",
    "            weight_decay=1e-6,\n",
    "            exclude_from_weight_decay=[\"batch_normalization\", \"bias\"],\n",
    "        )\n",
    "\n",
    "        # \"decay the learning rate with the cosine decay schedule without restarts\"\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer, epochs, eta_min=0, last_epoch=-1\n",
    "        )\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    return optimizer, scheduler\n",
    "\n",
    "def save_model(model, optimizer, scheduler, current_epoch, name):\n",
    "    out = os.path.join('./saved_models/',name.format(current_epoch))\n",
    "\n",
    "    torch.save({'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict':scheduler.state_dict()}, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S7_mTokQpl_j",
    "outputId": "661a7693-50a0-4668-d8a6-e3c0e7245767"
   },
   "outputs": [],
   "source": [
    "# !mkdir /content/saved_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# matern_kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4zSuiZ5zpl_j"
   },
   "source": [
    "#### Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P1Ileef6pl_j"
   },
   "outputs": [],
   "source": [
    "# class SimCLR_Loss(nn.Module):\n",
    "#     def __init__(self, batch_size, temperature, nu=1.5, length_scale=1.0):\n",
    "#         super(SimCLR_Loss, self).__init__()\n",
    "#         self.batch_size = batch_size\n",
    "#         self.temperature = temperature\n",
    "#         self.nu = nu  # Smoothness parameter for the Matérn kernel\n",
    "#         self.length_scale = length_scale  # Length scale parameter\n",
    "        \n",
    "#         self.mask = self.mask_correlated_samples(batch_size)\n",
    "#         self.criterion = nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "\n",
    "#     def mask_correlated_samples(self, batch_size):\n",
    "#         N = 2 * batch_size\n",
    "#         mask = torch.ones((N, N), dtype=bool)\n",
    "#         mask = mask.fill_diagonal_(0)\n",
    "        \n",
    "#         for i in range(batch_size):\n",
    "#             mask[i, batch_size + i] = 0\n",
    "#             mask[batch_size + i, i] = 0\n",
    "            \n",
    "#         return mask\n",
    "\n",
    "#     def matern_kernel(self, z: Tensor) -> Tensor:\n",
    "#         \"\"\"\n",
    "#         Computes the Matérn kernel matrix.\n",
    "        \n",
    "#         Parameters:\n",
    "#         - z: The input tensor of shape (N, D)\n",
    "        \n",
    "#         Returns:\n",
    "#         - kernel: The Matérn kernel similarity matrix\n",
    "#         \"\"\"\n",
    "#         # Compute pairwise Euclidean distances\n",
    "#         dist = torch.cdist(z, z, p=2)\n",
    "        \n",
    "#         # Matérn kernel computation\n",
    "#         term1 = (torch.sqrt(2 * self.nu) * dist / self.length_scale)\n",
    "#         term2 = torch.exp(-term1)\n",
    "#         term3 = (term1 ** self.nu) / gamma(self.nu)  # Note: gamma(self.nu) is the Gamma function\n",
    "        \n",
    "#         kernel = term3 * term2\n",
    "#         return kernel\n",
    "\n",
    "#     def forward(self, z_i, z_j):\n",
    "#         \"\"\"\n",
    "#         Compute the loss using Matérn kernel similarity.\n",
    "#         \"\"\"\n",
    "#         N = 2 * self.batch_size  # Number of total samples (2N)\n",
    "\n",
    "#         z = torch.cat((z_i, z_j), dim=0)  # Combine the feature representations\n",
    "\n",
    "#         # Compute Matérn kernel similarity\n",
    "#         sim = self.matern_kernel(z) / self.temperature\n",
    "\n",
    "#         sim_i_j = torch.diag(sim, self.batch_size)\n",
    "#         sim_j_i = torch.diag(sim, -self.batch_size)\n",
    "        \n",
    "#         # Positive and negative samples\n",
    "#         positive_samples = torch.cat((sim_i_j, sim_j_i), dim=0).reshape(N, 1)\n",
    "#         negative_samples = sim[self.mask].reshape(N, -1)\n",
    "        \n",
    "#         # Labels for cross-entropy loss\n",
    "#         labels = torch.zeros(N, dtype=torch.long, device=positive_samples.device)\n",
    "        \n",
    "#         # Combine positive and negative samples\n",
    "#         logits = torch.cat((positive_samples, negative_samples), dim=1)\n",
    "#         loss = self.criterion(logits, labels)\n",
    "#         loss /= N\n",
    "        \n",
    "#         return loss\n",
    "\n",
    "class SimCLR_Loss(nn.Module):\n",
    "    def __init__(self, batch_size, temperature):\n",
    "        super(SimCLR_Loss, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.temperature = temperature\n",
    "        \n",
    "        self.mask = self.mask_correlated_samples(batch_size)\n",
    "        self.criterion = nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "        \n",
    "    def mask_correlated_samples(self, batch_size):\n",
    "        N = 2 * batch_size\n",
    "        mask = torch.ones((N, N), dtype=bool)\n",
    "        mask = mask.fill_diagonal_(0)\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            mask[i, batch_size + i] = 0\n",
    "            mask[batch_size + i, i] = 0\n",
    "            \n",
    "        return mask\n",
    "\n",
    "    def concatenation_sum_kernel(self, z: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Computes the concatenation sum kernel matrix.\n",
    "        \n",
    "        Parameters:\n",
    "        - z: The input tensor of shape (N, D)\n",
    "        \n",
    "        Returns:\n",
    "        - kernel: The concatenation sum kernel similarity matrix\n",
    "        \"\"\"\n",
    "        N, D = z.size()\n",
    "        \n",
    "        # Concatenate the tensor with itself\n",
    "        z_concat = torch.cat((z, z), dim=1)  # Shape (N, 2D)\n",
    "        \n",
    "        # Compute pairwise dot products\n",
    "        dot_product = torch.mm(z_concat, z_concat.t())\n",
    "        \n",
    "        return dot_product\n",
    "\n",
    "    def forward(self, z_i, z_j):\n",
    "        \"\"\"\n",
    "        Compute the loss using concatenation sum kernel similarity.\n",
    "        \"\"\"\n",
    "        N = 2 * self.batch_size  # Number of total samples (2N)\n",
    "\n",
    "        z = torch.cat((z_i, z_j), dim=0)  # Combine the feature representations\n",
    "\n",
    "        # Compute concatenation sum kernel similarity\n",
    "        sim = self.concatenation_sum_kernel(z) / self.temperature\n",
    "\n",
    "        sim_i_j = torch.diag(sim, self.batch_size)\n",
    "        sim_j_i = torch.diag(sim, -self.batch_size)\n",
    "        \n",
    "        # Positive and negative samples\n",
    "        positive_samples = torch.cat((sim_i_j, sim_j_i), dim=0).reshape(N, 1)\n",
    "        negative_samples = sim[self.mask].reshape(N, -1)\n",
    "        \n",
    "        # Labels for cross-entropy loss\n",
    "        labels = torch.zeros(N, dtype=torch.long, device=positive_samples.device)\n",
    "        \n",
    "        # Combine positive and negative samples\n",
    "        logits = torch.cat((positive_samples, negative_samples), dim=1)\n",
    "        loss = self.criterion(logits, labels)\n",
    "        loss /= N\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z2XOwfizpl_j"
   },
   "source": [
    "#### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lC1atJNhpl_k"
   },
   "outputs": [],
   "source": [
    "from torch.optim.optimizer import Optimizer, required\n",
    "import re\n",
    "\n",
    "EETA_DEFAULT = 0.001\n",
    "\n",
    "\n",
    "class LARS(Optimizer):\n",
    "    \"\"\"\n",
    "    Layer-wise Adaptive Rate Scaling for large batch training.\n",
    "    Introduced by \"Large Batch Training of Convolutional Networks\" by Y. You,\n",
    "    I. Gitman, and B. Ginsburg. (https://arxiv.org/abs/1708.03888)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        params,\n",
    "        lr=required,\n",
    "        momentum=0.9,\n",
    "        use_nesterov=False,\n",
    "        weight_decay=0.0,\n",
    "        exclude_from_weight_decay=None,\n",
    "        exclude_from_layer_adaptation=None,\n",
    "        classic_momentum=True,\n",
    "        eeta=EETA_DEFAULT,\n",
    "    ):\n",
    "        \"\"\"Constructs a LARSOptimizer.\n",
    "        Args:\n",
    "        lr: A `float` for learning rate.\n",
    "        momentum: A `float` for momentum.\n",
    "        use_nesterov: A 'Boolean' for whether to use nesterov momentum.\n",
    "        weight_decay: A `float` for weight decay.\n",
    "        exclude_from_weight_decay: A list of `string` for variable screening, if\n",
    "            any of the string appears in a variable's name, the variable will be\n",
    "            excluded for computing weight decay. For example, one could specify\n",
    "            the list like ['batch_normalization', 'bias'] to exclude BN and bias\n",
    "            from weight decay.\n",
    "        exclude_from_layer_adaptation: Similar to exclude_from_weight_decay, but\n",
    "            for layer adaptation. If it is None, it will be defaulted the same as\n",
    "            exclude_from_weight_decay.\n",
    "        classic_momentum: A `boolean` for whether to use classic (or popular)\n",
    "            momentum. The learning rate is applied during momeuntum update in\n",
    "            classic momentum, but after momentum for popular momentum.\n",
    "        eeta: A `float` for scaling of learning rate when computing trust ratio.\n",
    "        name: The name for the scope.\n",
    "        \"\"\"\n",
    "\n",
    "        self.epoch = 0\n",
    "        defaults = dict(\n",
    "            lr=lr,\n",
    "            momentum=momentum,\n",
    "            use_nesterov=use_nesterov,\n",
    "            weight_decay=weight_decay,\n",
    "            exclude_from_weight_decay=exclude_from_weight_decay,\n",
    "            exclude_from_layer_adaptation=exclude_from_layer_adaptation,\n",
    "            classic_momentum=classic_momentum,\n",
    "            eeta=eeta,\n",
    "        )\n",
    "\n",
    "        super(LARS, self).__init__(params, defaults)\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.weight_decay = weight_decay\n",
    "        self.use_nesterov = use_nesterov\n",
    "        self.classic_momentum = classic_momentum\n",
    "        self.eeta = eeta\n",
    "        self.exclude_from_weight_decay = exclude_from_weight_decay\n",
    "        # exclude_from_layer_adaptation is set to exclude_from_weight_decay if the\n",
    "        # arg is None.\n",
    "        if exclude_from_layer_adaptation:\n",
    "            self.exclude_from_layer_adaptation = exclude_from_layer_adaptation\n",
    "        else:\n",
    "            self.exclude_from_layer_adaptation = exclude_from_weight_decay\n",
    "\n",
    "    def step(self, epoch=None, closure=None):\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        if epoch is None:\n",
    "            epoch = self.epoch\n",
    "            self.epoch += 1\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            weight_decay = group[\"weight_decay\"]\n",
    "            momentum = group[\"momentum\"]\n",
    "            eeta = group[\"eeta\"]\n",
    "            lr = group[\"lr\"]\n",
    "\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "\n",
    "                param = p.data\n",
    "                grad = p.grad.data\n",
    "\n",
    "                param_state = self.state[p]\n",
    "\n",
    "                # TODO: get param names\n",
    "                # if self._use_weight_decay(param_name):\n",
    "                grad += self.weight_decay * param\n",
    "\n",
    "                if self.classic_momentum:\n",
    "                    trust_ratio = 1.0\n",
    "\n",
    "                    # TODO: get param names\n",
    "                    # if self._do_layer_adaptation(param_name):\n",
    "                    w_norm = torch.norm(param)\n",
    "                    g_norm = torch.norm(grad)\n",
    "\n",
    "                    device = g_norm.get_device()\n",
    "                    trust_ratio = torch.where(\n",
    "                        w_norm.gt(0),\n",
    "                        torch.where(\n",
    "                            g_norm.gt(0),\n",
    "                            (self.eeta * w_norm / g_norm),\n",
    "                            torch.Tensor([1.0]).to(device),\n",
    "                        ),\n",
    "                        torch.Tensor([1.0]).to(device),\n",
    "                    ).item()\n",
    "\n",
    "                    scaled_lr = lr * trust_ratio\n",
    "                    if \"momentum_buffer\" not in param_state:\n",
    "                        next_v = param_state[\"momentum_buffer\"] = torch.zeros_like(\n",
    "                            p.data\n",
    "                        )\n",
    "                    else:\n",
    "                        next_v = param_state[\"momentum_buffer\"]\n",
    "\n",
    "                    next_v.mul_(momentum).add_(scaled_lr, grad)\n",
    "                    if self.use_nesterov:\n",
    "                        update = (self.momentum * next_v) + (scaled_lr * grad)\n",
    "                    else:\n",
    "                        update = next_v\n",
    "\n",
    "                    p.data.add_(-update)\n",
    "                else:\n",
    "                    raise NotImplementedError\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def _use_weight_decay(self, param_name):\n",
    "        \"\"\"Whether to use L2 weight decay for `param_name`.\"\"\"\n",
    "        if not self.weight_decay:\n",
    "            return False\n",
    "        if self.exclude_from_weight_decay:\n",
    "            for r in self.exclude_from_weight_decay:\n",
    "                if re.search(r, param_name) is not None:\n",
    "                    return False\n",
    "        return True\n",
    "\n",
    "    def _do_layer_adaptation(self, param_name):\n",
    "        \"\"\"Whether to do layer-wise learning rate adaptation for `param_name`.\"\"\"\n",
    "        if self.exclude_from_layer_adaptation:\n",
    "            for r in self.exclude_from_layer_adaptation:\n",
    "                if re.search(r, param_name) is not None:\n",
    "                    return False\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hPRKldHepl_k",
    "outputId": "4b152adf-55cb-4114-a8fa-dd35f9243f86"
   },
   "outputs": [],
   "source": [
    "optimizer = LARS(\n",
    "    [params for params in model.parameters() if params.requires_grad],\n",
    "    lr=0.2,\n",
    "    weight_decay=1e-6,\n",
    "    exclude_from_weight_decay=[\"batch_normalization\", \"bias\"],\n",
    ")\n",
    "\n",
    "# \"decay the learning rate with the cosine decay schedule without restarts\"\n",
    "warmupscheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lambda epoch : (epoch+1)/10.0, verbose = True)\n",
    "mainscheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, 500, eta_min=0.05, last_epoch=-1, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AHlV6dPaZClA"
   },
   "outputs": [],
   "source": [
    "criterion = SimCLR_Loss(batch_size = 128, temperature = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B08GiwN2pl_l"
   },
   "outputs": [],
   "source": [
    "trimages = images[:40000]\n",
    "valimages = images[40000:]\n",
    "trlabels = labels[:40000]\n",
    "vallabels = labels[40000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SbKVH47BUmeu"
   },
   "outputs": [],
   "source": [
    "dg = C10DataGen('train',trimages)#train_df)\n",
    "dl = DataLoader(dg,batch_size = 128,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rKYK4PXwpl_l"
   },
   "outputs": [],
   "source": [
    "vdg = C10DataGen('valid',valimages)#_df)\n",
    "vdl = DataLoader(vdg,batch_size = 128,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 333
    },
    "id": "Hs2MG-9fL65Z",
    "outputId": "2524b588-8d8c-41bf-aabd-a6393fb6596c"
   },
   "outputs": [],
   "source": [
    "plt.hist(vallabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OJPC4wYwpl_l"
   },
   "source": [
    "#### Real Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f_FOfbMRpl_m"
   },
   "outputs": [],
   "source": [
    "\n",
    "nr = 0\n",
    "global_step = 0\n",
    "current_epoch = 0\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oz09Sza8pl_n"
   },
   "outputs": [],
   "source": [
    "def plot_features(model, num_classes, num_feats, batch_size, val_df = None):\n",
    "    preds = np.array([]).reshape((0,1))\n",
    "    gt = np.array([]).reshape((0,1))\n",
    "    feats = np.array([]).reshape((0,num_feats))\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x1,x2 in vdl:\n",
    "            x1 = x1.squeeze().to(device = 'cuda:0', dtype = torch.float)#.view((-1,3,224,224))\n",
    "            #y = y.to(device = 'cuda:0')#.view((-1,1))\n",
    "            out = model(x1)\n",
    "            out = out.cpu().data.numpy()#.reshape((1,-1))\n",
    "            feats = np.append(feats,out,axis = 0)\n",
    "    \n",
    "    tsne = TSNE(n_components = 2, perplexity = 50)\n",
    "    x_feats = tsne.fit_transform(feats)\n",
    "    #plt.scatter(x_feats[:,1],x_feats[:,0])\n",
    "    num_samples = int(batch_size*(valimages.shape[0]//batch_size))#(len(val_df)\n",
    "    \n",
    "    for i in range(num_classes):\n",
    "        #plt.scatter(x_feats[val_df['class'].iloc[:num_samples].values==i,1],x_feats[val_df['class'].iloc[:num_samples].values==i,0])\n",
    "        plt.scatter(x_feats[vallabels[:num_samples]==i,1],x_feats[vallabels[:num_samples]==i,0])\n",
    "    \n",
    "    plt.legend([str(i) for i in range(num_classes)])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UdumxB3Gc_IG"
   },
   "outputs": [],
   "source": [
    "# original_model = model.module\n",
    "\n",
    "# Now you can call the function with the correct attributes\n",
    "# plot_features(original_model.pretrained, 10, 2048, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eVPcOEDqpl_m"
   },
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, optimizer):\n",
    "    loss_epoch = 0\n",
    "    \n",
    "    for step, (x_i, x_j) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        x_i = x_i.squeeze().to('cuda:0').float()\n",
    "        x_j = x_j.squeeze().to('cuda:0').float()\n",
    "\n",
    "        # positive pair, with encoding\n",
    "        z_i = model(x_i)\n",
    "        z_j = model(x_j)\n",
    "\n",
    "        loss = criterion(z_i, z_j)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "        if nr == 0 and step % 50 == 0:\n",
    "            print(f\"Step [{step}/{len(train_loader)}]\\t Loss: {round(loss.item(), 5)}\")\n",
    "\n",
    "        loss_epoch += loss.item()\n",
    "    return loss_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o9q6ZHKkpl_m"
   },
   "outputs": [],
   "source": [
    "def valid(valid_loader, model, criterion):\n",
    "    loss_epoch = 0\n",
    "    for step, (x_i, x_j) in enumerate(valid_loader):\n",
    "        \n",
    "        x_i = x_i.squeeze().to('cuda:0').float()\n",
    "        x_j = x_j.squeeze().to('cuda:0').float()\n",
    "\n",
    "        # positive pair, with encoding\n",
    "        z_i = model(x_i)\n",
    "        z_j = model(x_j)\n",
    "\n",
    "        loss = criterion(z_i, z_j)\n",
    "        \n",
    "        if nr == 0 and step % 50 == 0:\n",
    "            print(f\"Step [{step}/{len(valid_loader)}]\\t Loss: {round(loss.item(),5)}\")\n",
    "\n",
    "        loss_epoch += loss.item()\n",
    "    return loss_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KSkx3X6Dpl_m"
   },
   "outputs": [],
   "source": [
    "tr_loss = []\n",
    "val_loss = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "tv6V3d-iZIbH",
    "outputId": "52c19fde-af32-4120-fa39-82091406a4df",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(10):\n",
    "        \n",
    "    print(f\"Epoch [{epoch}/{epochs}]\\t\")\n",
    "    stime = time.time()\n",
    "\n",
    "    model.train()\n",
    "    tr_loss_epoch = train(dl, model, criterion, optimizer)\n",
    "\n",
    "    if nr == 0 and epoch < 10:\n",
    "        warmupscheduler.step()\n",
    "    if nr == 0 and epoch >= 10:\n",
    "        mainscheduler.step()\n",
    "    \n",
    "    lr = optimizer.param_groups[0][\"lr\"]\n",
    "\n",
    "    if nr == 0 and (epoch+1) % 50 == 0:\n",
    "        save_model(model, optimizer, mainscheduler, current_epoch,\"SimCLR_CIFAR10_RN50_P128_LR0P2_LWup10_Cos500_T0p5_B128_checkpoint_{}_260621.pt\")\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss_epoch = valid(vdl, model, criterion)\n",
    "\n",
    "    if nr == 0:\n",
    "        \n",
    "        tr_loss.append(tr_loss_epoch / len(dl))\n",
    "        \n",
    "        val_loss.append(val_loss_epoch / len(vdl))\n",
    "        \n",
    "        print(\n",
    "            f\"Epoch [{epoch}/{epochs}]\\t Training Loss: {tr_loss_epoch / len(dl)}\\t lr: {round(lr, 5)}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"Epoch [{epoch}/{epochs}]\\t Validation Loss: {val_loss_epoch / len(vdl)}\\t lr: {round(lr, 5)}\"\n",
    "        )\n",
    "        current_epoch += 1\n",
    "\n",
    "    dg.on_epoch_end()\n",
    "\n",
    "    time_taken = (time.time()-stime)/60\n",
    "    print(f\"Epoch [{epoch}/{epochs}]\\t Time Taken: {time_taken} minutes\")\n",
    "\n",
    "    if (epoch+1)%10==0:\n",
    "        plot_features(model.pretrained, 10, 2048, 128) #, valimages)\n",
    "\n",
    "## end training\n",
    "save_model(model, optimizer, mainscheduler, current_epoch, \"SimCLR_CIFAR10_RN50_P128_LR0P2_LWup10_Cos500_T0p5_B128_checkpoint_{}_260621.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-QVpNB6PG1SV"
   },
   "outputs": [],
   "source": [
    "np.unique(vallabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cs31qGP3ak8z"
   },
   "outputs": [],
   "source": [
    "save_model(model, optimizer, mainscheduler, current_epoch, \"SimCLR_IMgNet200_RN50_P512_LR0P5_B128_checkpoint_{}_140621.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "UkqOUp1-pl_n",
    "outputId": "2b2db8c3-61a0-44b9-e474-60ed1ac3ae4d"
   },
   "outputs": [],
   "source": [
    "plt.plot(tr_loss,'b-')\n",
    "plt.plot(val_loss,'r-')\n",
    "plt.legend(['t','v'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VxN3ptCspl_n"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ejhbdm9Dpl_o"
   },
   "source": [
    "# DOWNSTREAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dbbwWRpfpl_o"
   },
   "source": [
    "## mODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h_Wp1rU1pl_o"
   },
   "outputs": [],
   "source": [
    "class DSModel(nn.Module):\n",
    "    def __init__(self, premodel, num_classes):\n",
    "        super(DSModel, self).__init__()\n",
    "        self.premodel = premodel.module  # Access the underlying model\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # Freeze the parameters of the premodel\n",
    "        for p in self.premodel.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        # Freeze the parameters of the projector\n",
    "        for p in self.premodel.projector.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        self.lastlayer = nn.Linear(128, self.num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.premodel(x)\n",
    "        x = self.lastlayer(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oXrsm4AJpl_o"
   },
   "outputs": [],
   "source": [
    "set_seed(16)\n",
    "dsmodel = DSModel(model, 10).to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q1J07OQEpl_o"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LF-rnMvdpl_o"
   },
   "source": [
    "### dATAgEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jpfLwM3apl_p"
   },
   "outputs": [],
   "source": [
    "NUM_FRAMES = 16\n",
    "NUM_CLASSES = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_yqAyDkz3yfJ"
   },
   "source": [
    "##### CIFAR 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-jZ5uHH33y73"
   },
   "outputs": [],
   "source": [
    "# Define the DSDataGen class\n",
    "class DSDataGen(Dataset):\n",
    "    def __init__(self, phase, imgarr, labels, num_classes=NUM_CLASSES):\n",
    "        self.phase = phase\n",
    "        self.num_classes = num_classes\n",
    "        self.imgarr = imgarr\n",
    "        self.labels = labels\n",
    "        self.indices = list(range(self.imgarr.shape[0]))\n",
    "        self.randomcrop = transforms.RandomResizedCrop(32, (0.8, 1.0))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.imgarr.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # DECLARE VARIABLES\n",
    "        x = self.imgarr[idx]\n",
    "        img = torch.from_numpy(x).float()\n",
    "            \n",
    "        # GET CLIP FRAMES\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # AUGMENT FRAMES\n",
    "        if self.phase == 'train':\n",
    "            img = self.randomcrop(img)\n",
    "\n",
    "        img = self.preprocess(img)\n",
    "        \n",
    "        return img, label\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        idx = random.sample(population=list(range(self.__len__())), k=self.__len__())\n",
    "        self.imgarr = self.imgarr[idx]\n",
    "        self.labels = self.labels[idx]\n",
    "        \n",
    "    def preprocess(self, frame):\n",
    "        frame = frame / 255.0\n",
    "        frame = (frame - MEAN) / STD\n",
    "        return frame\n",
    "\n",
    "# Initialize the dataset without the batch_size parameter\n",
    "dg = DSDataGen('train', trimages, trlabels, num_classes=NUM_CLASSES)\n",
    "\n",
    "# Create the DataLoader\n",
    "dl = DataLoader(dg, batch_size=32, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eMU6_6ZH45eQ"
   },
   "source": [
    "##### DATALOADERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nRmCSHLGpl_p"
   },
   "outputs": [],
   "source": [
    "# dg = DSDataGen('train', trimages,trlabels,\n",
    "#                batch_size=1, num_classes=NUM_CLASSES)\n",
    "\n",
    "# dl = DataLoader(dg,batch_size = 32, drop_last = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QPpdzzROpl_p"
   },
   "outputs": [],
   "source": [
    "vdg = DSDataGen('valid', valimages, vallabels, num_classes=NUM_CLASSES)\n",
    "\n",
    "# Create the DataLoader for the validation dataset\n",
    "vdl = DataLoader(vdg, batch_size=32, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "08AB0TF-pl_q"
   },
   "source": [
    "### LOSS weIGHTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fyr2NMaRpl_q",
    "outputId": "0d7cd6e9-05fb-444f-c926-44771e5d3f42"
   },
   "outputs": [],
   "source": [
    "dsoptimizer = torch.optim.SGD([params for params in dsmodel.parameters() if params.requires_grad],lr = 0.01, momentum = 0.9)\n",
    "\n",
    "#dsoptimizer = torch.optim.Adam([params for params in dsmodel.parameters() if params.requires_grad],lr=1e-5)\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(dsoptimizer, step_size=1, gamma=0.98, last_epoch=-1, verbose = True) #CosineAnnealingWarmRestarts(dsoptimizer,5,eta_min = 1e-6,last_epoch = -1, verbose = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Prr0RWkxpl_q"
   },
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LtLyVVd6pl_q"
   },
   "outputs": [],
   "source": [
    "tr_ep_loss = []\n",
    "tr_ep_acc = []\n",
    "tr_ep_auc = []\n",
    "val_ep_loss = []\n",
    "val_ep_acc = []\n",
    "val_ep_auc = []\n",
    "\n",
    "min_val_loss = 100.0\n",
    "\n",
    "batch_size = 1\n",
    "EPOCHS = 10\n",
    "num_cl = 10\n",
    "accumulation_steps = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hw2x1wXNpl_q",
    "outputId": "3d52272a-d015-4832-f944-a91f84874bf9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(20):\n",
    "    stime = time.time()\n",
    "    print(\"=============== Epoch : %3d ===============\" % (epoch + 1))\n",
    "    \n",
    "    loss_sublist = np.array([])\n",
    "    acc_sublist = np.array([])\n",
    "    \n",
    "    dsmodel.train()\n",
    "    \n",
    "    dsoptimizer.zero_grad()\n",
    "    \n",
    "    for x, y in dl:\n",
    "        x = x.squeeze().to(device='cuda:0', dtype=torch.float32)  # Ensure x is float32\n",
    "        y = y.to(device='cuda:0', dtype=torch.long)  # Ensure y is long\n",
    "        \n",
    "        z = dsmodel(x)\n",
    "        z = z.to(torch.float32)  # Ensure z is float32\n",
    "        \n",
    "        dsoptimizer.zero_grad()\n",
    "        \n",
    "        tr_loss = loss_fn(z, y)  # This should now work correctly\n",
    "        tr_loss.backward()\n",
    "\n",
    "        preds = torch.exp(z.cpu().data) / torch.sum(torch.exp(z.cpu().data))\n",
    "        \n",
    "        dsoptimizer.step()\n",
    "        \n",
    "        loss_sublist = np.append(loss_sublist, tr_loss.cpu().data)\n",
    "        acc_sublist = np.append(acc_sublist, np.array(np.argmax(preds, axis=1) == y.cpu().data.view(-1)).astype('int'), axis=0)\n",
    "    \n",
    "    print('ESTIMATING TRAINING METRICS.............')\n",
    "    print('TRAINING BINARY CROSSENTROPY LOSS: ', np.mean(loss_sublist))\n",
    "    print('TRAINING BINARY ACCURACY: ', np.mean(acc_sublist))\n",
    "    \n",
    "    tr_ep_loss.append(np.mean(loss_sublist))\n",
    "    tr_ep_acc.append(np.mean(acc_sublist))\n",
    "    \n",
    "    print('ESTIMATING VALIDATION METRICS.............')\n",
    "    \n",
    "    dsmodel.eval()\n",
    "    \n",
    "    loss_sublist = np.array([])\n",
    "    acc_sublist = np.array([])\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in vdl:\n",
    "            x = x.squeeze().to(device='cuda:0', dtype=torch.float32)  # Ensure x is float32\n",
    "            y = y.to(device='cuda:0', dtype=torch.long)  # Ensure y is long\n",
    "            z = dsmodel(x)\n",
    "            z = z.to(torch.float32)  # Ensure z is float32\n",
    "\n",
    "            val_loss = loss_fn(z, y)\n",
    "\n",
    "            preds = torch.exp(z.cpu().data) / torch.sum(torch.exp(z.cpu().data))\n",
    "\n",
    "            loss_sublist = np.append(loss_sublist, val_loss.cpu().data)\n",
    "            acc_sublist = np.append(acc_sublist, np.array(np.argmax(preds, axis=1) == y.cpu().data.view(-1)).astype('int'), axis=0)\n",
    "            \n",
    "    print('VALIDATION BINARY CROSSENTROPY LOSS: ', np.mean(loss_sublist))\n",
    "    print('VALIDATION BINARY ACCURACY: ', np.mean(acc_sublist))\n",
    "    \n",
    "    val_ep_loss.append(np.mean(loss_sublist))\n",
    "    val_ep_acc.append(np.mean(acc_sublist))\n",
    "    \n",
    "    lr_scheduler.step()\n",
    "    \n",
    "    dg.on_epoch_end()\n",
    "    \n",
    "    if np.mean(loss_sublist) <= min_val_loss:\n",
    "        min_val_loss = np.mean(loss_sublist)\n",
    "        print('Saving model...')\n",
    "        torch.save({'model_state_dict': dsmodel.state_dict(),\n",
    "                    'optimizer_state_dict': dsoptimizer.state_dict()},\n",
    "                   './saved_models/cifar10_rn50_p128_sgd0p01_decay0p98_all_lincls_300621.pt')\n",
    "    \n",
    "    print(\"Time Taken : %.2f minutes\" % ((time.time() - stime) / 60.0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "JvOAcf8Spl_s",
    "outputId": "9fef888d-8995-49df-e02b-52816b664e9f"
   },
   "outputs": [],
   "source": [
    "plt.plot([t for t in tr_ep_acc])\n",
    "plt.plot([t for t in val_ep_acc])\n",
    "plt.legend(['train','valid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "XtjgIJbmpl_s",
    "outputId": "4a08f09d-65d7-45a3-be52-fb865762c984"
   },
   "outputs": [],
   "source": [
    "plt.plot(tr_ep_loss)\n",
    "plt.plot(val_ep_loss)\n",
    "plt.legend(['train','valid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3EafnbO9pl_s",
    "outputId": "e0eaeb53-06b8-44bd-9873-9f903836ea12"
   },
   "outputs": [],
   "source": [
    "# plt.plot([t for t in tr_ep_auc])\n",
    "# plt.plot([t for t in val_ep_auc])\n",
    "# plt.legend(['train','valid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LvBasUGYQ6oZ"
   },
   "outputs": [],
   "source": [
    "# tdg = DSDataGen('test', testimages, testlabels, num_classes=10)\n",
    "\n",
    "# tdl = DataLoader(tdg, batch_size = 32, drop_last = True)\n",
    "\n",
    "# dsmodel.eval()\n",
    "    \n",
    "# loss_sublist = np.array([])\n",
    "# acc_sublist = np.array([])\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for x,y in vdl:\n",
    "#         x = x.squeeze().to(device = 'cuda:0', dtype = torch.float)\n",
    "#         y = y.to(device = 'cuda:0')\n",
    "#         z = dsmodel(x)\n",
    "\n",
    "#         val_loss = loss_fn(z,y)\n",
    "\n",
    "#         preds = torch.exp(z.cpu().data)/torch.sum(torch.exp(z.cpu().data))\n",
    "\n",
    "#         loss_sublist = np.append(loss_sublist, val_loss.cpu().data)\n",
    "#         acc_sublist = np.append(acc_sublist,np.array(np.argmax(preds,axis=1)==y.cpu().data.view(-1)).astype('int'),axis=0)\n",
    "\n",
    "# print('TEST BINARY CROSSENTROPY LOSS: ',np.mean(loss_sublist))\n",
    "# print('TEST BINARY ACCURACY: ',np.mean(acc_sublist))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "NwHbQkep1MLx",
    "uxVU4yN2fmNf",
    "ziLMBZNX5M38",
    "zB7FcvNU5TyH",
    "yS0S5_Qr5fKm",
    "K6kf2F-K5pw7",
    "6qoGDc-05wOG",
    "fhlUTwvc53jw",
    "1QF5TgaoUQgz",
    "C969RA6x3zKK"
   ],
   "name": "SImCLR cifar10.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:rp] *",
   "language": "python",
   "name": "conda-env-rp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "625px",
    "width": "382px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "159070707f784605a2336cffb647d02f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6a7b6912ecb3418da2aa22ef34d9ae5a",
      "placeholder": "​",
      "style": "IPY_MODEL_f47647ab20454613b70c4f78c6175c7b",
      "value": " 97.8M/97.8M [46:13&lt;00:00, 37.0kB/s]"
     }
    },
    "1a700b2fc234400498581d2081c89153": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6213f8efc85b41b8b9c23375c416057e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_67a457df909442e58782494206d86974",
       "IPY_MODEL_159070707f784605a2336cffb647d02f"
      ],
      "layout": "IPY_MODEL_1a700b2fc234400498581d2081c89153"
     }
    },
    "67a457df909442e58782494206d86974": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c3777ffbc9f941e5a52e11a7e2a59ec0",
      "max": 102530333,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a38a8031fa3f45feaa930bc12f2efc28",
      "value": 102530333
     }
    },
    "6a7b6912ecb3418da2aa22ef34d9ae5a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a38a8031fa3f45feaa930bc12f2efc28": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "c3777ffbc9f941e5a52e11a7e2a59ec0": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f47647ab20454613b70c4f78c6175c7b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
